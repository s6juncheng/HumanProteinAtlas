{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hpa_src.models.loss import CrossEntropyLossOneHot, FocalLoss\n",
    "from torch.nn import BCELoss, BCEWithLogitsLoss\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate targets, assume every sample has two labels\n",
    "np.random.seed(110)\n",
    "targets = np.random.randint(0, 28, (100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand to 28 dimension\n",
    "onehot_targets = np.zeros((100, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_targets[np.arange(100), targets[:,1][np.newaxis]] = 1\n",
    "onehot_targets[np.arange(100), targets[:,0][np.newaxis]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(110)\n",
    "predictions = np.random.normal(0, 1, (100,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATADIR = '/s/project/junTemp/HPAIC/'\n",
    "image_df = pd.read_csv(DATADIR + \"data/png/train.csv\")\n",
    "image_df['target_list'] = image_df['Target'].map(lambda x: [int(a) for a in x.split(' ')])\n",
    "import itertools\n",
    "all_labels = np.array(list(itertools.chain(*image_df.target_list.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_n = np.unique(all_labels, return_counts=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12885,  1254,  3621,  1561,  1858,  2513,  1008,  2822,    53,\n",
       "          45,    28,  1093,   688,   537,  1066,    21,   530,   210,\n",
       "         902,  1482,   172,  3777,   802,  2965,   322,  8228,   328,\n",
       "          11])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = class_n / sum(class_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.losses import binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = K.eval(K.sigmoid(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7970609"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(binary_crossentropy(K.variable(onehot_targets), K.variable(pred_prob))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.from_numpy(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_targets = torch.from_numpy(onehot_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/modules/i12g/anaconda/3-5.0.1/envs/hpaic/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "bcelogit = BCEWithLogitsLoss()\n",
    "bce = BCELoss()\n",
    "ce = CrossEntropyLossOneHot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7971, dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bcelogit(predictions, onehot_targets, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.7854, dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(ce(predictions, onehot_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, \n",
    "                 alpha=1, \n",
    "                 gamma=2, \n",
    "                 logits=False, \n",
    "                 reduction='elementwise_mean'):\n",
    "        \n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none') # -log(p_t), p_t: predicted target prob\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * torch.pow((1-pt), self.gamma) * BCE_loss\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return F_loss\n",
    "        elif self.reduction == 'elementwise_mean':\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "focal = FocalLoss(gamma=0, logits=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0293, dtype=torch.float64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "focal(predictions, onehot_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_with_logits(input, target, weight=None, size_average=None,\n",
    "                                     reduce=None, reduction='elementwise_mean', pos_weight=None):\n",
    "    r\"\"\"Function that measures Binary Cross Entropy between target and output\n",
    "    logits.\n",
    "\n",
    "    See :class:`~torch.nn.BCEWithLogitsLoss` for details.\n",
    "\n",
    "    Args:\n",
    "        input: Tensor of arbitrary shape\n",
    "        target: Tensor of the same shape as input\n",
    "        weight (Tensor, optional): a manual rescaling weight\n",
    "            if provided it's repeated to match input tensor shape\n",
    "        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
    "            the losses are averaged over each loss element in the batch. Note that for\n",
    "            some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
    "            is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
    "            when reduce is ``False``. Default: ``True``\n",
    "        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
    "            losses are averaged or summed over observations for each minibatch depending\n",
    "            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
    "            batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            'none' | 'elementwise_mean' | 'sum'. 'none': no reduction will be applied,\n",
    "            'elementwise_mean': the sum of the output will be divided by the number of\n",
    "            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`. Default: 'elementwise_mean'\n",
    "        pos_weight (Tensor, optional): a weight of positive examples.\n",
    "                Must be a vector with length equal to the number of classes.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "         >>> input = torch.randn(3, requires_grad=True)\n",
    "         >>> target = torch.empty(3).random_(2)\n",
    "         >>> loss = F.binary_cross_entropy_with_logits(input, target)\n",
    "         >>> loss.backward()\n",
    "    \"\"\"\n",
    "    if size_average is not None or reduce is not None:\n",
    "        reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "    if not (target.size() == input.size()):\n",
    "        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\n",
    "\n",
    "    max_val = (-input).clamp(min=0)\n",
    "\n",
    "    if pos_weight is None:\n",
    "        loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "    else:\n",
    "        log_weight = 1 + (pos_weight - 1) * target\n",
    "        loss = input - input * target + log_weight * (max_val + ((-max_val).exp() + (-input - max_val).exp()).log())\n",
    "\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    if reduction == 'none':\n",
    "        return loss\n",
    "    elif reduction == 'elementwise_mean':\n",
    "        return loss.mean()\n",
    "    else:\n",
    "        return loss.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hpaic]",
   "language": "python",
   "name": "conda-env-hpaic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
